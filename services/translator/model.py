"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SP6M8cT7WqVwRrISmPlC19nHlow84Mhi
"""
import typing as t
import os
import re
import unicodedata

# import tensorflow as tf
from keras_transformer import get_model, decode


def create_translator(source: str, target: str, input: str) -> str:
    base_path = os.path.dirname(os.path.abspath(__file__))
    path = f"{base_path}/translations/{source}_{target}"

    def parse(path: str):
        with open(path, mode="r", encoding="utf-8-sig") as f:
            lines = f.read().split("\n")[:-1]

        sources = []
        targets = []

        for line in lines:
            source, target = line.split(",")
            sources.append(source.split(" "))
            targets.append(target.split(" "))

        return sources[: int(len(sources) * 0.7)], targets[: int(len(sources) * 0.7)]

    source_tokens, target_tokens = parse(f"{path}/dataset.txt")
    len(source_tokens)

    # Generate dictionaries
    def build_token_dict(token_list):
        token_dict = {
            "<PAD>": 0,
            "<START>": 1,
            "<END>": 2,
        }
        for tokens in token_list:
            for token in tokens:
                if token not in token_dict:
                    token_dict[token] = len(token_dict)
        return token_dict

    source_token_dict = build_token_dict(source_tokens)
    target_token_dict = build_token_dict(target_tokens)
    target_token_dict_inv = {v: k for k, v in target_token_dict.items()}

    print(source_token_dict)

    # Add special tokens
    encode_tokens = [["<START>"] + tokens + ["<END>"] for tokens in source_tokens]
    decode_tokens = [["<START>"] + tokens + ["<END>"] for tokens in target_tokens]
    output_tokens = [tokens + ["<END>", "<PAD>"] for tokens in target_tokens]

    # Padding
    source_max_len = max(map(len, encode_tokens))
    target_max_len = max(map(len, decode_tokens))

    encode_tokens = [
        tokens + ["<PAD>"] * (source_max_len - len(tokens)) for tokens in encode_tokens
    ]
    decode_tokens = [
        tokens + ["<PAD>"] * (target_max_len - len(tokens)) for tokens in decode_tokens
    ]
    output_tokens = [
        tokens + ["<PAD>"] * (target_max_len - len(tokens)) for tokens in output_tokens
    ]

    encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens]
    decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]
    decode_output = [
        list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens
    ]

    # Build & fit model
    model = get_model(
        token_num=max(len(source_token_dict), len(target_token_dict)),
        embed_dim=32,
        encoder_num=2,
        decoder_num=2,
        head_num=4,
        hidden_dim=128,
        dropout_rate=0.05,
        use_same_embed=True,  # Use different embeddings for different languages
    )

    model.compile("adam", "sparse_categorical_crossentropy", metrics=["accuracy"])
    model.summary()

    # logger = tf.keras.callbacks.CSVLogger(f"{path}/out/history.csv", append=True)

    # checkpoint = tf.keras.callbacks.ModelCheckpoint(
    #     filepath=f"{path}/out/weights",
    #     save_weights_only=True,
    #     save_best_only=True,
    #     save_freq="epoch",
    #     monitor="loss",
    #     mode="auto",
    #     verbose=1,
    # )

    # model.fit(
    #     x=[np.array(encode_input * 1), np.array(decode_input * 1)],
    #     y=np.array(decode_output * 1),
    #     epochs=15,
    #     batch_size=32,
    #     callbacks=[checkpoint],
    # )

    model.load_weights(f"{path}/out/weights")

    def unicode_to_ascii(s):
        return "".join(
            c for c in unicodedata.normalize("NFD", s) if unicodedata.category(c) != "Mn"
        )

    # Lowercase, trim, and remove non-letter characters

    def normalize(s):
        s = unicode_to_ascii(s.lower().strip())
        s = re.sub(r"([.!?])", r" \1", s)
        s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
        return s

    # Predict
    sentence_tokens = [tokens + ["<END>", "<PAD>"] for tokens in [normalize(input).split(" ")]]
    tr_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in sentence_tokens][0]

    decoded = decode(
        model,
        tr_input,
        start_token=target_token_dict["<START>"],
        end_token=target_token_dict["<END>"],
        pad_token=target_token_dict["<PAD>"],
    )

    return "{}".format(" ".join(map(lambda x: target_token_dict_inv[x], decoded[1:-1])))
